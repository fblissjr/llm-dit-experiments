[project]
name = "llm-dit-experiments"
version = "0.5.0"
description = "LLM-DiT experimentation platform with pluggable backends"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    # PyTorch with CUDA 13.0 - see [tool.uv.sources] for index config
    "torch",
    "torchvision",
    # Z-Image-Turbo requires diffusers 0.36+ with ZImagePipeline
    "diffusers @ git+https://github.com/huggingface/diffusers",
    # transformers v5 compatible (uses quantization_config instead of load_in_8bit/load_in_4bit)
    # Works with both v4.40+ and v5.0+
    "transformers>=4.40.0",
    "accelerate",
    "safetensors",
    "pillow",
    "numpy",  # Used by experiments/compare/diff.py for image difference calculations
    "pyyaml",
    "tomli; python_version < '3.11'",  # TOML config support
    "bitsandbytes; sys_platform != 'darwin'",  # 8-bit quantization (Linux/CUDA only)
    "httpx",  # API backend support
    "uvicorn>=0.38.0",
    "fastapi>=0.123.4",
    "python-multipart>=0.0.20",
    "einops",  # Tensor reshaping for Qwen-Image models
    "image-reward>=1.5",
    "matplotlib>=3.10.7",
]

[project.optional-dependencies]
dev = [
    "ipython",
    "jupyter",
    "pytest",
    "ruff",
]
analysis = [
    "matplotlib>=3.7",
    "seaborn>=0.12",
    "scikit-learn>=1.3",  # For t-SNE, PCA, clustering
    "umap-learn>=0.5",
]
training = [
    "peft>=0.8.0",  # LoRA adapters
    "pandas>=2.0.0",  # CSV dataset loading
    "tqdm>=4.65.0",  # Progress bars
    "tensorboard>=2.15.0",  # Optional logging
]
# Note: vllm and sglang require CUDA, will be added back when needed
# vllm = ["vllm"]
# sglang = ["sglang"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/llm_dit"]

[tool.hatch.metadata]
allow-direct-references = true

# Optional source builds (not in dependencies):
#   SageAttention: ./scripts/install_sageattention.sh
#   FlashAttention: pip install flash-attn --no-build-isolation

[[tool.uv.index]]
name = "pytorch-cu130"
url = "https://download.pytorch.org/whl/cu130"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-cu130" }
torchvision = { index = "pytorch-cu130" }

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "W"]
ignore = ["E501"]  # Line too long handled by formatter

[dependency-groups]
dev = [
    "pytest>=9.0.1",
    "pytest-asyncio>=1.3.0",
]
